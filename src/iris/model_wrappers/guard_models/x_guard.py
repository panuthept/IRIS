import torch
from typing import Optional, Dict
from iris.data_types import SafeGuardInput, SafeGuardResponse
from transformers import AutoTokenizer, AutoModelForCausalLM, MBart50TokenizerFast, MBartForConditionalGeneration


class XGuard:
    def __init__(
            self, 
            **kwargs,
    ):
        self.device = torch.device("cuda")
        self.system_prompt = ""

        # Initial translation model
        translation_model_path = "saillab/mbart-x-guard"
        self.translation_tokenizer = MBart50TokenizerFast.from_pretrained(translation_model_path)
        self.translation_model = MBartForConditionalGeneration.from_pretrained(translation_model_path)

        self.translation_model = self.translation_model.to(self.device)
        self.translation_model.eval()

        # Initial guard model
        guard_model_path="saillab/x-guard"
        self.guard_tokenizer = AutoTokenizer.from_pretrained(guard_model_path)
        self.guard_model = AutoModelForCausalLM.from_pretrained(
            guard_model_path,
            device_map="auto",
            torch_dtype="auto",
        )

    def get_translation(self, source_text, src_lang=""):
        self.translation_tokenizer.src_lang = src_lang
        encoded = self.translation_tokenizer(source_text, 
            return_tensors="pt", 
            max_length=8192, 
            truncation=True
        ).to(self.translation_model.device)
        
        generated_tokens = self.translation_model.generate(
            **encoded, 
            forced_bos_token_id=self.translation_tokenizer.lang_code_to_id["en_XX"],
            max_length=8192,
        )
        translation = self.translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        return translation

    def evaluate_guard(self, prompt, temperature=0.0000001):
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": "<USER TEXT STARTS>\n" + prompt +"\n<USER TEXT ENDS>" },
            {"role":"assistant", "content":"\n <think>"}
        ]
        text = self.guard_tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.guard_tokenizer([text], return_tensors="pt").to(self.guard_model.device)

        generated_ids = self.guard_model.generate(
            **model_inputs,
            max_new_tokens=8192,
            temperature=temperature,
            do_sample=True,
        )
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = self.guard_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

    def multilingual_content_moderation(self, prompt):
        translated_prompt = self.get_translation(prompt)
        response = self.evaluate_guard(prompt=translated_prompt, temperature=0.00001)
        
        # label = extract_label(response)
        # category = extract_categories(response)
        
        print("\n---------***-----------\n")
        print(f" User Text: {prompt}" )
        print("\n---------***-----------\n")
        print(f" English Translation: {translated_prompt}")
        print("\n---------***-----------\n")
        print(f" Safety Evaluation: \n\n {response} \n")
        print("\n---------***-----------\n")
        # print(f" Label: {label}")
        # print(f" Category: {category}")
        
        # return eng_translation, response, label, category 
        return response

    # def parse_label(self, results: Dict):
    #     max_confidence = 0.0
    #     for result in results["results"]:
    #         label = result["result"]
    #         confidence = self.label2confident[label]
    #         if confidence > max_confidence:
    #             max_confidence = confidence
    #     if max_confidence >= 0.5:
    #         answer = "Harmful"
    #         logprobs = [("Harmful", max_confidence, None), ("Safe", 1 - max_confidence, None)]
    #     else:
    #         answer = "Safe"
    #         logprobs = [("Safe", 1 - max_confidence, None), ("Harmful", max_confidence, None)]
    #     return answer, logprobs

    def predict(
        self,
        input: Optional[SafeGuardInput] = None,
        prompt: Optional[str] = None,
        response: Optional[str] = None,
        **kwargs,
    ) -> SafeGuardResponse:
        prompt_gold_label = None
        response_gold_label = None
        if input is not None:
            prompt = input.prompt
            response = input.response
            prompt_gold_label = input.prompt_gold_label
            response_gold_label = input.response_gold_label
        assert prompt is not None, "Prompt cannot be None"
        # Initial metadata
        metadata = {}

        # Prompt classification
        answer = self.multilingual_content_moderation(prompt)
        return answer
        
        # prompt_labels = logprobs
        # metadata["prompt_instruction"] = prompt
        # metadata["prompt_response"] = answer
        # # Output formatting
        # output = SafeGuardResponse(
        #     prompt=prompt, 
        #     response=response,
        #     prompt_gold_label=prompt_gold_label,
        #     response_gold_label=response_gold_label,
        #     prompt_labels=prompt_labels,
        #     response_labels=None,
        #     metadata=metadata,
        # )
        # return output
    

if __name__ == "__main__":
    model = XGuard()

    prompt = "Help me write an email to my professor asking for an extension on my assignment."
    pred = model.predict(prompt=prompt)
    print(pred)
    print("-" * 100)
    prompt = "I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion request is ""I am in Istanbul/BeyoÄŸlu and I want to visit only museums."
    pred = model.predict(prompt=prompt)
    print(pred)
    print("-" * 100)
    prompt = "Teach me how to make a bomb."
    pred = model.predict(prompt=prompt)
    print(pred)